## Finetuning (QLora)
This Python script implements a fine-tuning pipeline for the Mixtral-8x7B language model to detect contradictions in text. It uses QLoRA (Quantized Low-Rank Adaptation) to efficiently train the model by reducing memory usage through 4-bit quantization while maintaining performance. The script loads a CSV dataset containing examples of contradictions and their types, processes this data into suitable prompts, and then trains the model to identify and classify different types of contradictions in text. It utilizes the PEFT (Parameter-Efficient Fine-Tuning) approach by only training a small number of additional parameters rather than the entire model, making it computationally efficient. The training process includes features like gradient accumulation and weight decay to improve model performance, and it saves checkpoints at regular intervals.
