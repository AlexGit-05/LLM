{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-16T19:29:47.757713Z","iopub.status.busy":"2024-04-16T19:29:47.757432Z","iopub.status.idle":"2024-04-16T19:30:33.195927Z","shell.execute_reply":"2024-04-16T19:30:33.195141Z","shell.execute_reply.started":"2024-04-16T19:29:47.757686Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\n","libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\n","libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n","momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n","spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n","ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\n","ydata-profiling 4.6.4 requires scipy<1.12,>=1.4.1, but you have scipy 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]},{"name":"stderr","output_type":"stream","text":["2024-04-16 19:30:24.610420: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-16 19:30:24.610519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-16 19:30:24.728540: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["#Installing and Loading the required libraries \n","!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy\n","\n","import torch\n","import transformers\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments\n","from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel # modules for QLoRA and PEFT\n","from datasets import load_dataset\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T19:30:33.198652Z","iopub.status.busy":"2024-04-16T19:30:33.197849Z","iopub.status.idle":"2024-04-16T19:45:09.235894Z","shell.execute_reply":"2024-04-16T19:45:09.234858Z","shell.execute_reply.started":"2024-04-16T19:30:33.198618Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ddbeb972dab44dd9628a9c52655caa3","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d6aa35e786b54ecf99ee860eb896b5fa","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8fa928d2200a44ca915edad2885fc2c4","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"545187162c244788934d96f8f84c6fea","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"67854a146be141ada1eed4431b9b37f8","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ff53714a6c9455998bed342b8dd0fda","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/92.7k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b16b913c443455199d4f4043897c90b","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f292d5f7912431abba97a889e6cfb3a","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00019.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3459c3803a340b3a8434ecbf44d854d","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a7c3f1a6a80e445cb0f2be53058acf94","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2b2a0bd9f5442be9864793d5b057525","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"65f9c5509d8143f0b872c6d34efd57dd","version_major":2,"version_minor":0},"text/plain":["model-00005-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dfef5a0495d34399a078f3e482d36b29","version_major":2,"version_minor":0},"text/plain":["model-00006-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b87a756396a47acbc6a01a1201b529d","version_major":2,"version_minor":0},"text/plain":["model-00007-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6cd190a094454dc387b13d180c334f7c","version_major":2,"version_minor":0},"text/plain":["model-00008-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6535918df58e448baf1a574fe0339142","version_major":2,"version_minor":0},"text/plain":["model-00009-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e962e6683a8445a8602978efeea9385","version_major":2,"version_minor":0},"text/plain":["model-00010-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da213a5313924da39b75d03eeb7159a0","version_major":2,"version_minor":0},"text/plain":["model-00011-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c4ee0712634a41359e0b4b0b1a69216c","version_major":2,"version_minor":0},"text/plain":["model-00012-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3524c33f38384986b076893153b5cd8d","version_major":2,"version_minor":0},"text/plain":["model-00013-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"823f767205314d2f999b25c498f64160","version_major":2,"version_minor":0},"text/plain":["model-00014-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b8e9e662403a49da8e82127d6ac4be03","version_major":2,"version_minor":0},"text/plain":["model-00015-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"362fa83c10c644488cd29eb93863e424","version_major":2,"version_minor":0},"text/plain":["model-00016-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d63b12858b5d4d55a3911968b2319922","version_major":2,"version_minor":0},"text/plain":["model-00017-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68617ce02e5a4ee2a60b8dd0683ba8bd","version_major":2,"version_minor":0},"text/plain":["model-00018-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d6904152ca946f6b769f24c125c0bc1","version_major":2,"version_minor":0},"text/plain":["model-00019-of-00019.safetensors:   0%|          | 0.00/4.22G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2fb51dd9e2904c66944bb12f10cb6a2a","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0b944bfbf784c2b8c9058cc0abbeeda","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["#Loading the pretrained model\n","#pre-trained language model to be loaded from Hugging Face's model hub\n","llm_model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n","\n","#tokenizer corresponding to the model to convert text into a format that the model can understand\n","tokenizer = AutoTokenizer.from_pretrained(llm_model)\n","\n","# Configure quantization settings using bitsandbytes library (QLoRa configarations)\n","# This setup is for loading the model with 4-bit weights using NF4 quantization\n","# NF4 is a specific quantization technique optimized for the model\n","# `load_in_4bit`: Enables loading models in 4-bit precision\n","# `bnb_4bit_quant_type`: Sets the type of 4-bit quantization (NF4 in this case)\n","# `bnb_4bit_use_double_quant`: Enables double quantization to further reduce memory usage\n","# `bnb_4bit_compute_dtype`: Sets the computation data type to bfloat16, balancing performance and precision\n","double_quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","\n","# Load the pre-trained causal language model with the specified quantization configuration\n","# `device_map=\"auto\"`: Automatically assigns model layers to available GPUs, optimizing for performance\n","model = AutoModelForCausalLM.from_pretrained(\n","    llm_model,\n","    quantization_config=double_quant_config, \n","    device_map=\"auto\"\n",")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T19:45:09.237566Z","iopub.status.busy":"2024-04-16T19:45:09.237281Z","iopub.status.idle":"2024-04-16T19:45:10.060110Z","shell.execute_reply":"2024-04-16T19:45:10.059287Z","shell.execute_reply.started":"2024-04-16T19:45:09.237542Z"},"trusted":true},"outputs":[],"source":["#Preparing the Model for QLoRA\n","# Prepare the model for training with quantization (such as k-bit quantization) to enhance efficiency\n","model = prepare_model_for_kbit_training(model)\n","\n","# Defined constants for the LoRA configuration:\n","CUTOFF_LEN = 256  #The maximum sequence length for processing\n","LORA_R = 8 # The rank of the low-rank matrices, controlling the amount of parameters to train\n","LORA_ALPHA = 2 * LORA_R # scaling factor for the low-rank matrices, calculated as twice the rank\n","LORA_DROPOUT = 0.1 #Dropout rate to prevent overfitting in the added LoRA layers\n","\n","# Configure the LoRA settings specific to the model’s needs:\n","# `r`: Rank of the adaptation matrix\n","# `lora_alpha`: Scaling factor for adaptation\n","# `target_modules`: Specifies which layers (MoE layers here) are targeted for low-rank parameterization\n","# `lora_dropout`: Dropout rate applied to LoRA parameters\n","# `bias`: Specifies if biases should be adapted; 'none' indicates no bias adaptation\n","# `task_type`: Defines the type of task the model is being prepared for, here it is causal language modeling\n","\n","config = LoraConfig(\n","    r=LORA_R,\n","    lora_alpha=LORA_ALPHA,\n","    target_modules=[ \"w1\"],  #just targetting the MoE layers.\n","    lora_dropout=LORA_DROPOUT,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","# Apply the PEFT model configuration to the prepared model\n","model = get_peft_model(model, config)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T19:45:10.063537Z","iopub.status.busy":"2024-04-16T19:45:10.062821Z","iopub.status.idle":"2024-04-16T19:45:10.104703Z","shell.execute_reply":"2024-04-16T19:45:10.103718Z","shell.execute_reply.started":"2024-04-16T19:45:10.063499Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 37748736 || all params: 23520350208 || trainable%: 0.16049393680864704\n"]}],"source":["# the number of trainable parameters, total parameters, and the percentage of parameters that are trainable in a model\n","def print_trainable_parameters(m):\n","    trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n","    all_params = sum(p.numel() for p in m.parameters())\n","    print(f\"trainable params: {trainable_params} || all params: {all_params} || trainable%: {100 * trainable_params / all_params}\")\n","\n","print_trainable_parameters(model)"]},{"cell_type":"markdown","metadata":{},"source":["Mixtral 8x7B outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.\n","\n","Mixtral-8x7B is the second large language model (LLM) released by mistral.ai, after Mistral-7B."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T19:45:10.106361Z","iopub.status.busy":"2024-04-16T19:45:10.105999Z","iopub.status.idle":"2024-04-16T19:45:10.486763Z","shell.execute_reply":"2024-04-16T19:45:10.485777Z","shell.execute_reply.started":"2024-04-16T19:45:10.106316Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17d63be195b84ad5acecaabf3db75ca9","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'Type': 'Implicit Contradiction:',\n"," 'Contradiction': ' \"The government claims to prioritize environmental protection.\",  \"Yet, they continue to approve projects that harm ecosystems.\"'}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# loading data from local directory in csv format\n","data = load_dataset('csv', data_files='/kaggle/input/d/alexwanjai/contradictions/Contradiction.csv')\n","\n","# loading data from hugging face datasets\n","#data = load_dataset(\"harpreetsahota/modern-to-shakesperean-translation\")\n","# extracting traning data\n","train_data = data['train']\n","# train_data view\n","train_data[10]"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T19:45:10.488345Z","iopub.status.busy":"2024-04-16T19:45:10.488005Z","iopub.status.idle":"2024-04-16T19:45:10.624985Z","shell.execute_reply":"2024-04-16T19:45:10.624083Z","shell.execute_reply.started":"2024-04-16T19:45:10.488318Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5963783cf5314d81a444ae21ffd78008","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/54 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Dataset({\n","    features: ['input_ids', 'attention_mask'],\n","    num_rows: 54\n","})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# use a common character as a padding token, '!' is used temporarily as a placeholder\n","tokenizer.pad_token = \"!\" #Not EOS, will explain another time.\n","\n","#Loading and Preparing the Dataset\n","#a function to generate a text prompt for translation tasks\n","def generate_prompt(user_query,  sep=\"\\n\\n### \"):  \n","    # a system message that instructs what the task is\n","    sys_msg= \"Check if tere is any contrudiction in the depostion and highlight the type of contrudiction.\"\n","    # Create the prompt by combining the instruction with the user's modern text input and the corresponding Shakespearean translation\n","    # This format helps in guiding the model to perform the translation task\n","    p =  \" [INST]\" + sys_msg +\"\\n\"+ user_query[\"Contradiction\"] + \"[/INST]\" +  user_query[\"Type\"] + \"\"\n","    return p\n","\n","# a function to tokenize the generated prompt\n","def tokenize(prompt):\n","    # Tokenize the prompt and append the End Of String token\n","    # Apply truncation to limit the prompts to a maximum length and pad any shorter prompts to this length\n","    return tokenizer(\n","        prompt + tokenizer.eos_token, # Add the end-of-sequence token to each prompt to signal the end of text\n","        truncation=True, # Ensure that prompts longer than the max length are truncated to fit\n","        max_length=CUTOFF_LEN ,# Set the maximum prompt length (CUTOFF_LEN must be defined elsewhere in your script)\n","        padding=\"max_length\" # Pad shorter prompts to ensure consistent length for batch processing\n","    )\n","\n","# Shuffle the training data and apply the tokenize function to each example\n","# Remove the original columns 'modern' and 'shakespearean' to prevent leakage and reduce memory usage\n","dataset = train_data.shuffle().map(lambda x: tokenize(generate_prompt(x)),remove_columns=[\"Contradiction\" , \"Type\"])\n","\n","# Output the processed dataset\n","dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T19:45:10.626633Z","iopub.status.busy":"2024-04-16T19:45:10.626268Z","iopub.status.idle":"2024-04-16T19:45:10.631051Z","shell.execute_reply":"2024-04-16T19:45:10.630021Z","shell.execute_reply.started":"2024-04-16T19:45:10.626598Z"},"trusted":true},"outputs":[],"source":["#dataset[1]"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T19:45:10.633297Z","iopub.status.busy":"2024-04-16T19:45:10.632606Z","iopub.status.idle":"2024-04-16T19:45:10.670196Z","shell.execute_reply":"2024-04-16T19:45:10.669447Z","shell.execute_reply.started":"2024-04-16T19:45:10.633262Z"},"trusted":true},"outputs":[],"source":["#Setting Up the Training Arguments\n","training_args = TrainingArguments(\n","    per_device_train_batch_size=1,  # Number of samples to process before moving to the next batch, per device\n","    gradient_accumulation_steps=4,  # Number of samples to process before moving to the next batch, per device\n","    num_train_epochs=6,  # Total number of training epochs (full passes over the training dataset)\n","    learning_rate=1e-4, # Learning rate for the optimizer\n","    logging_steps=2, # Frequency of logging predictions (every 2 steps)\n","    optim=\"adamw_torch\", # Specify optimizer type, here it's AdamW as implemented in PyTorch\n","    save_strategy=\"epoch\", # Strategy to save the model checkpoint ('epoch' to save at the end of each epoch)\n","    weight_decay=0.01, # Strength of weight decay\n","    output_dir=\"mixtral-contradiction_detector\" # Directory where the training outputs (models, logs) will be saved\n",")\n","\n","# These arguments will guide how the model is trained, including how often it updates the weights,\n","# how it handles batches of data, and where it saves its outputs."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T19:45:10.671619Z","iopub.status.busy":"2024-04-16T19:45:10.671337Z","iopub.status.idle":"2024-04-16T19:45:11.350178Z","shell.execute_reply":"2024-04-16T19:45:11.349150Z","shell.execute_reply.started":"2024-04-16T19:45:10.671594Z"},"trusted":true},"outputs":[],"source":["#Training the Model\n","trainer = Trainer( # The Trainer handles the training and evaluation of the model.\n","    model=model,# The pre-trained model that you want to fine-tune.\n","    train_dataset=dataset,# The dataset to use for training, prepared earlier.\n","    args=training_args,# Training arguments set previously, defining the training parameters.\n","    data_collator=transformers.DataCollatorForLanguageModeling(\n","        tokenizer, # The tokenizer used for pre-processing text data.\n","        mlm=False # Specifies that this is not a Masked Language Model (MLM) training.\n","    ),\n",")\n","\n","# Disable caching to potentially reduce memory usage during traininga and\n","# speed up training by storing past computations but uses more memory.\n","model.config.use_cache = False"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T19:45:11.352924Z","iopub.status.busy":"2024-04-16T19:45:11.352628Z","iopub.status.idle":"2024-04-16T21:39:35.350565Z","shell.execute_reply":"2024-04-16T21:39:35.349493Z","shell.execute_reply.started":"2024-04-16T19:45:11.352897Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.16.6 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240416_202245-cdqn3m36</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/alexwaweru/huggingface/runs/cdqn3m36/workspace' target=\"_blank\">devout-surf-6</a></strong> to <a href='https://wandb.ai/alexwaweru/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/alexwaweru/huggingface' target=\"_blank\">https://wandb.ai/alexwaweru/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/alexwaweru/huggingface/runs/cdqn3m36/workspace' target=\"_blank\">https://wandb.ai/alexwaweru/huggingface/runs/cdqn3m36/workspace</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [78/78 1:15:31, Epoch 5/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>2</td>\n","      <td>7.074000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>5.757100</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>4.759900</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>4.090500</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>3.364700</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>2.770000</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>2.226100</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>1.928900</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>1.687200</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.525800</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>1.430200</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>1.264500</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>1.078700</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.993700</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.918500</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.962200</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.842500</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.830700</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.795500</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.818300</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.827100</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.740600</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.738600</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.615700</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.722600</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.669200</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.620000</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.670700</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.627100</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.576600</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.611000</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.614700</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.641100</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.575100</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.549800</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.598500</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>0.538000</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.587700</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>0.563500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=78, training_loss=1.4668368849998865, metrics={'train_runtime': 6863.5174, 'train_samples_per_second': 0.047, 'train_steps_per_second': 0.011, 'total_flos': 2.233674925867008e+16, 'train_loss': 1.4668368849998865, 'epoch': 5.78})"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Training\n","trainer.train()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4811442,"sourceId":8138540,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
