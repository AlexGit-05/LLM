{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8587773,"sourceType":"datasetVersion","datasetId":5136595}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Install dependencies","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T11:03:32.021060Z","iopub.execute_input":"2024-06-03T11:03:32.021623Z","iopub.status.idle":"2024-06-03T11:03:32.398720Z","shell.execute_reply.started":"2024-06-03T11:03:32.021596Z","shell.execute_reply":"2024-06-03T11:03:32.397809Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"562ea40bb8134fbda1bed010337cf1c5"}},"metadata":{}}]},{"cell_type":"code","source":"import shutil\n\n# Define the paths for aiohttp files\naiohttp_path = '/opt/conda/lib/python3.10/site-packages/aiohttp*'\naiohttp_dist_info_path = '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info'\n\n# Remove aiohttp package directories and files\nfor path in [aiohttp_path, aiohttp_dist_info_path]:\n    shutil.rmtree(path, ignore_errors=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T11:04:54.064835Z","iopub.execute_input":"2024-06-03T11:04:54.065542Z","iopub.status.idle":"2024-06-03T11:04:54.071138Z","shell.execute_reply.started":"2024-06-03T11:04:54.065505Z","shell.execute_reply":"2024-06-03T11:04:54.070125Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#!pip install -r -q requirements.txt\n\n!pip -q install torch==2.2.0\n!pip -q install typer==0.9.1\n\n!pip -q install pyannote.audio # diarization\n!pip -q install pydub # segmentation\n!pip -q install transformers # transcription\n\n!pip install modelbit","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:24:22.500487Z","iopub.execute_input":"2024-06-03T12:24:22.501134Z","iopub.status.idle":"2024-06-03T12:24:22.505125Z","shell.execute_reply.started":"2024-06-03T12:24:22.501101Z","shell.execute_reply":"2024-06-03T12:24:22.504204Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#from transcription import Recording, Transcription\n\nfrom huggingface_hub import HfApi\navailable_pipelines = [p.modelId for p in HfApi().list_models(filter=\"pyannote-audio-pipeline\")]\nlist(filter(lambda p: p.startswith(\"pyannote/\"), available_pipelines))\n\nfrom pyannote.audio import Pipeline\npipeline_diar = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=True)\n\nimport torch\nimport torchaudio\nimport base64\nfrom io import BytesIO\n\nfrom pyannote.core import Annotation\n\nfrom pydub import AudioSegment\n\nfrom transformers import pipeline\n\ntranscriber = pipeline(model=\"openai/whisper-large-v2\", return_timestamps=False)","metadata":{"ExecuteTime":{"end_time":"2024-03-24T15:36:44.791581663Z","start_time":"2024-03-24T15:36:44.768528837Z"},"execution":{"iopub.status.busy":"2024-06-03T11:11:33.799762Z","iopub.execute_input":"2024-06-03T11:11:33.800086Z","iopub.status.idle":"2024-06-03T11:12:51.631145Z","shell.execute_reply.started":"2024-06-03T11:11:33.800054Z","shell.execute_reply":"2024-06-03T11:12:51.630341Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.yaml:   0%|          | 0.00/469 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e967d356249d4ce7b160dc788cdd0797"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/5.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c60b0cf7cd44b57a270b281289a8f23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.yaml:   0%|          | 0.00/399 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e240eb3621c4849bf9cfbe91274f10e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/26.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8d39e3f085f43ef889274886f78b91e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.yaml:   0%|          | 0.00/221 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8558a48a1004aea9ff4681a1e683360"}},"metadata":{}},{"name":"stderr","text":"2024-06-03 11:11:56.492127: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-03 11:11:56.492271: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-03 11:11:56.724470: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.99k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5683d51f0014d0eaa78127559e26b6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d80776aa1e5a4c6588f1eb0f0f383456"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/4.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b879ce06f0b46dbba09cbc67433a3fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f16b6e3019b40918d111c4f68b49cd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4db4e81c483c4f64baa535be60181f83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c6b7218fc35485d982971cb3f9f7f07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a0a62aa9d384b069c6f43ac84a98146"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44c6bec9208b416ab0e57553ff750379"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7df76c542a93445e90431c074d7ff677"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"271b024d0a1c4c9f993583e27698c1c7"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f163e4ebaf54944a570d20ec6a18bbd"}},"metadata":{}}]},{"cell_type":"markdown","source":"#### Log into the modelbit workspace","metadata":{}},{"cell_type":"code","source":"import modelbit\nmb = modelbit.login()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T11:15:35.038025Z","iopub.execute_input":"2024-06-03T11:15:35.038786Z","iopub.status.idle":"2024-06-03T11:15:35.289154Z","shell.execute_reply.started":"2024-06-03T11:15:35.038751Z","shell.execute_reply":"2024-06-03T11:15:35.288468Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"#### Training job","metadata":{}},{"cell_type":"markdown","source":"Dataset","metadata":{}},{"cell_type":"code","source":"AUDIO_FILE = f\"/kaggle/input/movie-audio/audio.mp3\"\n\npipeline_diar.to(torch.device(\"cuda\"))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T11:15:40.872072Z","iopub.execute_input":"2024-06-03T11:15:40.872421Z","iopub.status.idle":"2024-06-03T11:15:41.209688Z","shell.execute_reply.started":"2024-06-03T11:15:40.872394Z","shell.execute_reply":"2024-06-03T11:15:41.208738Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization at 0x7cda1041b610>"},"metadata":{}}]},{"cell_type":"code","source":"# Read the MP3 file in binary mode\nwith open(AUDIO_FILE, \"rb\") as file:\n    mp3_data = file.read()\n\n# Base64 encoded string\nencoded_data = base64.b64encode(mp3_data).decode('utf-8')\n\nprint(encoded_data[:100])","metadata":{"execution":{"iopub.status.busy":"2024-06-03T11:16:32.527618Z","iopub.execute_input":"2024-06-03T11:16:32.527953Z","iopub.status.idle":"2024-06-03T11:16:32.538074Z","shell.execute_reply.started":"2024-06-03T11:16:32.527927Z","shell.execute_reply":"2024-06-03T11:16:32.537022Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"SUQzBAAAAAABAFRYWFgAAAASAAADbWFqb3JfYnJhbmQAZGFzaABUWFhYAAAAEQAAA21pbm9yX3ZlcnNpb24AMABUWFhYAAAAHAAA\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The Pipeline","metadata":{}},{"cell_type":"code","source":"# # Decode the base64 encoded string\n# decoded_data = base64.b64decode(encoded_data)\n\n# # Write the decoded audio data to a temporary file buffer\n# # Create a BytesIO object from the byte array\n# with BytesIO(decoded_data) as audio_buffer:\n#     audio_buffer.seek(0)\n#     # Load audio with pydub for easy slicing\n#     audio = AudioSegment.from_file(audio_buffer)\n#     audio.export(\"out.mp3\", format=\"mp3\")\n\n#     # Run diarization pipeline\n#     dia = pipeline_diar(\"out.mp3\")\n#     assert isinstance(dia, Annotation)\n\n#     # Prepare a list to store data\n#     data = []\n    \n#     for i, (speech_turn, track, speaker) in enumerate(dia.itertracks(yield_label=True)):\n#         print(f\"{speech_turn.start:4.1f} {speech_turn.end:4.1f} {speaker}\")\n\n#         # Extract start and end times\n#         start_time, end_time = speech_turn.start, speech_turn.end\n\n#         # Extract the segment of the audio\n#         segment = audio[int(start_time*1000):int(end_time*1000)].export(\"out.mp3\", format=\"mp3\")\n#         text = transcriber(\"out.mp3\")['text']\n\n#         # Append the data to the list\n#         data.append({\n#             \"Start Time\": start_time,\n#             \"End Time\": end_time,\n#             \"Speaker\": speaker,\n#             \"Transcription\": text\n#             })\n#         print(data)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T10:25:38.780856Z","iopub.execute_input":"2024-06-03T10:25:38.781462Z","iopub.status.idle":"2024-06-03T10:25:38.786729Z","shell.execute_reply.started":"2024-06-03T10:25:38.781423Z","shell.execute_reply":"2024-06-03T10:25:38.785875Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"def transcribe_recording(base64_encoded_audio):\n    \"\"\"\n    Transcribes audio data given as base64 encoded byte string into text and yields\n    the transcribed text incrementally as it becomes available.\n\n    Parameters:\n    - base64_audio (str): Base64 encoded byte string of audio data.\n\n    Yields:\n    - str: Transcribed text incrementally as it becomes available.\n    \"\"\"\n    # Decode the base64 encoded string\n    base64_decoded_audio = base64.b64decode(base64_encoded_audio)\n    \n    with BytesIO(base64_decoded_audio) as audio_buffer:\n        audio_buffer.seek(0)\n        # Load audio with pydub for easy slicing\n        audio = AudioSegment.from_file(audio_buffer)\n        audio.export(\"out.mp3\", format=\"mp3\")\n\n        # Run diarization pipeline\n        dia = pipeline_diar(\"out.mp3\")\n        assert isinstance(dia, Annotation)\n        # Prepare a list to store data\n        data = []\n\n        for i, (speech_turn, track, speaker) in enumerate(dia.itertracks(yield_label=True)):\n            # Extract start and end times\n            start_time, end_time = speech_turn.start, speech_turn.end\n\n            # Extract and transcribe the audio segment\n            segment_audio = audio[int(start_time * 1000):int(end_time * 1000)]\n            segment_audio.export(\"out.mp3\", format=\"mp3\")\n            text = transcriber(\"out.mp3\")['text']\n\n            # Append the data to the list\n            data.append({\n                \"Start Time\": start_time,\n                \"End Time\": end_time,\n                \"Speaker\": speaker,\n                \"Transcription\": text\n            })\n\n    return data\n","metadata":{"ExecuteTime":{"end_time":"2024-03-24T15:41:11.594794822Z","start_time":"2024-03-24T15:41:11.553449266Z"},"execution":{"iopub.status.busy":"2024-06-03T11:16:47.226562Z","iopub.execute_input":"2024-06-03T11:16:47.226914Z","iopub.status.idle":"2024-06-03T11:16:47.236169Z","shell.execute_reply.started":"2024-06-03T11:16:47.226884Z","shell.execute_reply":"2024-06-03T11:16:47.234758Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"transcribe_recording(encoded_data)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T11:16:52.786406Z","iopub.execute_input":"2024-06-03T11:16:52.787228Z","iopub.status.idle":"2024-06-03T11:26:00.959680Z","shell.execute_reply.started":"2024-06-03T11:16:52.787190Z","shell.execute_reply":"2024-06-03T11:26:00.958785Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[{'Start Time': 4.31721875,\n  'End Time': 8.04659375,\n  'Speaker': 'SPEAKER_00',\n  'Transcription': \" So last night I was watching this movie. Have you ever seen the movie North and South? I haven't.\"},\n {'Start Time': 7.64159375,\n  'End Time': 8.02971875,\n  'Speaker': 'SPEAKER_01',\n  'Transcription': ' I have it.'},\n {'Start Time': 8.04659375,\n  'End Time': 8.19846875,\n  'Speaker': 'SPEAKER_01',\n  'Transcription': ' you'},\n {'Start Time': 8.384093750000002,\n  'End Time': 11.489093750000002,\n  'Speaker': 'SPEAKER_00',\n  'Transcription': \" It's kind of like Pride and Prejudice. It's really good. I...\"},\n {'Start Time': 11.539718750000002,\n  'End Time': 14.42534375,\n  'Speaker': 'SPEAKER_01',\n  'Transcription': \" I haven't even seen Pride and Prejudice. I'm like way out of the loop on them.\"},\n {'Start Time': 12.872843750000001,\n  'End Time': 13.36221875,\n  'Speaker': 'SPEAKER_00',\n  'Transcription': ' Alors...'},\n {'Start Time': 14.492843750000002,\n  'End Time': 15.707843750000002,\n  'Speaker': 'SPEAKER_00',\n  'Transcription': ' Oh, that kind of stuff.'},\n {'Start Time': 15.809093750000002,\n  'End Time': 16.24784375,\n  'Speaker': 'SPEAKER_00',\n  'Transcription': ' 뭐...'},\n {'Start Time': 16.602218750000002,\n  'End Time': 18.340343750000002,\n  'Speaker': 'SPEAKER_00',\n  'Transcription': \" It's a good movie, I like it.\"},\n {'Start Time': 18.188468750000002,\n  'End Time': 19.63971875,\n  'Speaker': 'SPEAKER_01',\n  'Transcription': ' Well, I think I saw like a...'},\n {'Start Time': 20.60159375,\n  'End Time': 25.96784375,\n  'Speaker': 'SPEAKER_01',\n  'Transcription': \" I didn't see the actual one. I want to say I saw one that was similar to it, kind of a takeoff version.\"},\n {'Start Time': 26.05221875,\n  'End Time': 26.06909375,\n  'Speaker': 'SPEAKER_01',\n  'Transcription': ' you'},\n {'Start Time': 26.06909375,\n  'End Time': 27.098468750000002,\n  'Speaker': 'SPEAKER_00',\n  'Transcription': ' Oh, okay.'},\n {'Start Time': 26.355968750000002,\n  'End Time': 29.79846875,\n  'Speaker': 'SPEAKER_01',\n  'Transcription': \" Okay, that's like the closest thing I know about it. Oh\"},\n {'Start Time': 27.300968750000003,\n  'End Time': 27.75659375,\n  'Speaker': 'SPEAKER_00',\n  'Transcription': ' Cool!'},\n {'Start Time': 29.59596875,\n  'End Time': 33.22409375,\n  'Speaker': 'SPEAKER_00',\n  'Transcription': \" Oh, that's cool. Yeah, it's really good. It's one of my favorite movies.\"},\n {'Start Time': 33.22409375,\n  'End Time': 35.16471875,\n  'Speaker': 'SPEAKER_01',\n  'Transcription': ' think my wife would enjoy it.'},\n {'Start Time': 34.82721875,\n  'End Time': 38.52284375,\n  'Speaker': 'SPEAKER_00',\n  'Transcription': ' I bet your wife would really enjoy it if you rented it for her and maybe got like'},\n {'Start Time': 38.674718750000004,\n  'End Time': 41.89784375,\n  'Speaker': 'SPEAKER_00',\n  'Transcription': ' You guys could get some popcorn or something. I bet you would both really like it.'},\n {'Start Time': 41.89784375,\n  'End Time': 43.517843750000004,\n  'Speaker': 'SPEAKER_01',\n  'Transcription': ' Sounds like an okay idea.'},\n {'Start Time': 43.517843750000004,\n  'End Time': 43.97346875,\n  'Speaker': 'SPEAKER_00',\n  'Transcription': ' 영상편집 및 자막을 사용하였습니다.'},\n {'Start Time': 43.97346875,\n  'End Time': 44.63159375,\n  'Speaker': 'SPEAKER_01',\n  'Transcription': ' I think I could fit that.'},\n {'Start Time': 44.63159375,\n  'End Time': 44.665343750000005,\n  'Speaker': 'SPEAKER_00',\n  'Transcription': ' you'},\n {'Start Time': 44.682218750000004,\n  'End Time': 47.23034375,\n  'Speaker': 'SPEAKER_00',\n  'Transcription': ' Yeah. You know you kind of talk about like Jim Raffer-debt?'},\n {'Start Time': 47.314718750000004,\n  'End Time': 47.66909375,\n  'Speaker': 'SPEAKER_01',\n  'Transcription': ' Yeah.'},\n {'Start Time': 47.65221875,\n  'End Time': 52.10721875,\n  'Speaker': 'SPEAKER_00',\n  'Transcription': \" The guy in the movie is like Jim and Freddie. He's just the man. Like everyone loves him.\"},\n {'Start Time': 49.744718750000004,\n  'End Time': 52.782218750000006,\n  'Speaker': 'SPEAKER_01',\n  'Transcription': \" The man, like everyone loves it. Yeah. That's cool.\"},\n {'Start Time': 52.71471875,\n  'End Time': 53.06909375,\n  'Speaker': 'SPEAKER_00',\n  'Transcription': ' Yeah.'},\n {'Start Time': 53.06909375,\n  'End Time': 56.71409375,\n  'Speaker': 'SPEAKER_01',\n  'Transcription': \" I'd probably check it out then. That seems pretty reasonable.\"},\n {'Start Time': 56.86596875,\n  'End Time': 57.338468750000004,\n  'Speaker': 'SPEAKER_00',\n  'Transcription': ' Awesome.'},\n {'Start Time': 57.57471875,\n  'End Time': 57.99659375,\n  'Speaker': 'SPEAKER_01',\n  'Transcription': ' Great.'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Deploy the pipeline","metadata":{}},{"cell_type":"code","source":"# deploy transcription\nmb.deploy(transcribe_recording, \n          require_gpu=True, \n#           system_packages=[\"ffmpeg\"],\n         python_packages=[\"tensorflow==2.16.1\"],\n#                            \"pyannote.audio==3.2.0\",\n#                            \"pydub==0.25.1\",\n#                            \"transformers==4.40.2\"]\n         )","metadata":{"execution":{"iopub.status.busy":"2024-06-03T11:35:47.569603Z","iopub.execute_input":"2024-06-03T11:35:47.570375Z","iopub.status.idle":"2024-06-03T11:36:12.954665Z","shell.execute_reply.started":"2024-06-03T11:35:47.570327Z","shell.execute_reply":"2024-06-03T11:36:12.953792Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n  <div>\n    <span style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; font-weight: bold; color: #15803d;\">Deploying </span> <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">transcribe_recording</span>\n  </div>\n  \n  \n\n\n  <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; margin-top: 10px;\">\n    <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; font-weight: bold; color: #845B99;\">Heads up!</div>\n    <div>\n        \n        <div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; border-left: 1px solid #845B99; margin-bottom: 10px;\">\n\n    \n      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n        In your <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">mb.depoy</span> parameters you specified <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">tensorflow==2.16.1</span>.\n        However the version detected locally is <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">tensorflow==2.15.0</span>. Version mismatches can lead to unexpected\n        behavior.\n      </div>\n\n      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">To match your local environment to your deployment's environment, run:</div>\n      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; padding-left: 15px;\">\n        <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">!pip install tensorflow==2.16.1</span>\n      </div>\n\n      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">Alternatively, to match your deployment's environment to your local environment, include this version in your\n      <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">mb.deploy</span> call instead:</div>\n      <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; padding-left: 15px;\">\n        <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">mb.deploy(my_deploy_function, <b>python_packages=[\"tensorflow==2.15.0\"]</b>)</span>\n      </div>\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n  </div>\n      \n    </div>\n  </div>\n\n  \n\n\n  \n\n\n\n\n  <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">Uploading dependencies...</div>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div style=\"margin: 0; padding: 5px; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n  <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none; font-weight: bold; color: #15803d;\">Success!</div>\n  \n    <div style=\"margin: 0; padding: 0; line-height: 1.75; font-size: 14px; vertical-align: baseline; list-style: none; font-family: Roboto, Arial, sans-serif; background: none;\">\n      Deployment <span style=\"margin: 0; padding: 3px; line-height: 1.75; font-size: 13px; vertical-align: baseline; list-style: none; font-family: monospace; background: none; font-weight: 400; background-color: rgba(209, 213, 219, 0.2);\">transcribe_recording</span>\n      will be ready in  a couple minutes.\n    </div>\n  \n\n  <a href=\"https://ap-south-1.modelbit.com/w/juristone/main/deployments/transcribe_recording/apis\" target=\"_blank\" style=\"display: inline-block; margin-top: 12px;\" >\n    <div\n      style=\"display: inline-block; background-color: #845B99; border-radius: 0.375rem; color: white; cursor: pointer; font-size: 14px; font-weight: 700; padding: 8px 16px;\"\n      onmouseenter=\"this.style.background='#714488'\"\n      onmouseleave=\"this.style.background='#845B99'\"\n    >\n      View in Modelbit\n    </div>\n  </a>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"modelbit.get_inference(\n  region=\"ap-south-1\",\n  workspace=\"juristone\",\n  deployment=\"transcribe_recording\",\n  data=encoded_data\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:23:09.156077Z","iopub.execute_input":"2024-06-03T12:23:09.156725Z","iopub.status.idle":"2024-06-03T12:23:09.160651Z","shell.execute_reply.started":"2024-06-03T12:23:09.156687Z","shell.execute_reply":"2024-06-03T12:23:09.159704Z"},"trusted":true},"execution_count":20,"outputs":[]}]}